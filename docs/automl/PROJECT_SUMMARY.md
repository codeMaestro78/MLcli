# PROJECT SUMMARY: mlcli-toolkit v0.3.0

## ðŸŽ¯ Project Overview

**mlcli-toolkit** is a production-ready CLI toolkit for training, evaluating, and tracking Machine Learning and Deep Learning models. It provides a unified interface for experiment tracking, hyperparameter tuning, model explainability, and an interactive TUI.

## ðŸ“Š Current Architecture

### Tech Stack

| Component           | Technology                                            |
| ------------------- | ----------------------------------------------------- |
| Language            | Python 3.10+                                          |
| CLI Framework       | Typer + Rich                                          |
| ML Frameworks       | scikit-learn, TensorFlow, XGBoost, LightGBM, CatBoost |
| Serialization       | Pickle, Joblib, ONNX, SavedModel (TF), H5 (TF)        |
| Config              | JSON/YAML via ConfigLoader                            |
| Experiment Tracking | Custom JSON-based (mini-MLflow)                       |
| UI                  | Textual TUI                                           |

### Core Modules

```
mlcli/
â”œâ”€â”€ cli.py              # Main CLI (1602 lines) - train, tune, eval commands
â”œâ”€â”€ trainers/           # 15+ model trainers
â”‚   â”œâ”€â”€ base_trainer.py    # Abstract BaseTrainer class
â”‚   â”œâ”€â”€ logistic_trainer.py
â”‚   â”œâ”€â”€ svm_trainer.py
â”‚   â”œâ”€â”€ rf_trainer.py      # Random Forest
â”‚   â”œâ”€â”€ xgb_trainer.py     # XGBoost
â”‚   â”œâ”€â”€ lightgbm_trainer.py
â”‚   â”œâ”€â”€ catboost_trainer.py
â”‚   â”œâ”€â”€ tf_dnn_trainer.py  # TensorFlow DNN
â”‚   â”œâ”€â”€ tf_cnn_trainer.py  # TensorFlow CNN
â”‚   â”œâ”€â”€ tf_rnn_trainer.py  # TensorFlow RNN
â”‚   â”œâ”€â”€ clustering/        # KMeans, DBSCAN
â”‚   â””â”€â”€ anomaly/           # IsolationForest, OneClassSVM
â”œâ”€â”€ tuner/              # Hyperparameter tuning
â”‚   â”œâ”€â”€ base_tuner.py      # Abstract BaseTuner class
â”‚   â”œâ”€â”€ grid_tuner.py      # GridSearchCV
â”‚   â”œâ”€â”€ random_tuner.py    # RandomizedSearchCV
â”‚   â”œâ”€â”€ optuna_tuner.py    # Bayesian (TPE)
â”‚   â””â”€â”€ tuner_factory.py   # Factory pattern
â”œâ”€â”€ preprocessor/       # Data preprocessing
â”‚   â”œâ”€â”€ base_preprocessor.py
â”‚   â”œâ”€â”€ pipeline.py        # PreprocessingPipeline
â”‚   â”œâ”€â”€ scalers.py         # StandardScaler, MinMax, etc.
â”‚   â”œâ”€â”€ encoders.py        # Label, OneHot encoding
â”‚   â””â”€â”€ feature_selectors.py # SelectKBest, RFE, VarianceThreshold
â”œâ”€â”€ runner/             # Experiment tracking
â”‚   â””â”€â”€ experiment_tracker.py  # JSON-based run tracking
â”œâ”€â”€ explainer/          # Model explainability
â”‚   â”œâ”€â”€ shap_explainer.py
â”‚   â””â”€â”€ lime_explainer.py
â”œâ”€â”€ config/             # Configuration management
â”‚   â””â”€â”€ loader.py          # ConfigLoader for JSON/YAML
â”œâ”€â”€ ui/                 # Interactive TUI
â”‚   â””â”€â”€ tui.py
â””â”€â”€ utils/              # Utilities
    â”œâ”€â”€ registry.py        # Model auto-registration
    â”œâ”€â”€ metrics.py         # Compute metrics
    â””â”€â”€ io.py              # Data loading
```

## ðŸ”§ Current ML Workflow

### 1. Training Flow

```
Config (JSON/YAML)
    â†“
ConfigLoader.load()
    â†“
Registry.get_trainer(model_type)
    â†“
Trainer.train(X_train, y_train, X_val, y_val)
    â†“
ExperimentTracker.log_metrics()
    â†“
Trainer.save() â†’ [pickle, joblib, onnx, h5, savedmodel]
```

### 2. Tuning Flow

```
Config with param_space
    â†“
TunerFactory.create(method, param_space)
    â†“
Tuner.tune(trainer_class, X, y)
    â†“
[Grid|Random|Bayesian] Search
    â†“
Best params + Optional train_best model
```

### 3. Existing CLI Commands

- `mlcli train --config <file>` - Train a model
- `mlcli tune --config <file> --method <grid|random|bayesian>` - Hyperparameter tuning
- `mlcli eval --model <path> --data <path>` - Evaluate saved model
- `mlcli list-models` - List available models
- `mlcli list-runs` - Show experiment history
- `mlcli ui` - Launch interactive TUI

## ðŸ—ï¸ Design Patterns Used

1. **Abstract Base Class Pattern**

   - `BaseTrainer` â†’ All trainers inherit
   - `BaseTuner` â†’ All tuners inherit
   - `BasePreprocessor` â†’ All preprocessors inherit

2. **Registry Pattern** (`@register_model` decorator)

   - Auto-registration via decorators
   - Lazy loading for heavy dependencies (TensorFlow)
   - Metadata storage (framework, model_type, description)

3. **Factory Pattern**

   - `TunerFactory.create(method, param_space)` â†’ Tuner instance
   - `PreprocessorFactory` for preprocessors

4. **Pipeline Pattern**
   - `PreprocessingPipeline.add_step().fit_transform()`

## ðŸ“ˆ Key Observations for AutoML Integration

### Strengths (Leverage These)

1. âœ… Abstract base classes allow easy extension
2. âœ… Registry pattern enables dynamic model discovery
3. âœ… Existing tuner infrastructure (Grid/Random/Bayesian)
4. âœ… Experiment tracker ready for AutoML logging
5. âœ… Preprocessing pipeline supports chaining
6. âœ… ONNX export for model portability

### Gaps to Fill for AutoML

1. âŒ No automatic model selection
2. âŒ No automatic preprocessing selection
3. âŒ No feature engineering automation
4. âŒ No ensemble/stacking support
5. âŒ No data type inference
6. âŒ No time budget management
7. âŒ No early stopping for model search
8. âŒ No cross-model comparison reporting

## ðŸ“¦ Dependencies (from pyproject.toml)

### Core

- numpy>=1.24,<2.0
- pandas>=2.0
- scikit-learn>=1.0

### ML Frameworks

- tensorflow>=2.10
- xgboost>=1.7
- lightgbm>=4.0.0
- catboost>=1.2.7

### Tuning

- optuna>=3.0.0 (implicit via optuna_tuner)

### CLI/UI

- typer[all]>=0.7.0
- rich-click>=1.6.0
- textual>=0.40.0

### Serialization

- onnx>=1.14
- onnxruntime>=1.15
- skl2onnx>=1.14
- joblib>=1.1

---

_Document generated for AutoML Integration Planning_
_Date: Phase 0 - Project Understanding_
