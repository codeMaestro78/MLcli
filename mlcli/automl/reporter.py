"""
AutoML Reporter

Provides comprehensive reporting for AutoML runs including:
- Rich console output with tables and panels
- JSON export for programmatic access
- HTML reports for sharing and documentation
- Summary generation for quick insights

This module consumes outputs from AutoMLClassifier and
formats them for different output channels.

Usage:
    from mlcli.automl import AutoMLClassifier
    from mlcli.automl.reporter import AutoMLReporter

    automl = AutoMLClassifier(...)
    automl.fit(X, y)

    reporter = AutoMLReporter(automl)
    reporter.print_summary()
    reporter.save_json("report.json")
    reporter.save_html("report.html")
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import logging

logger = logging.getLogger(__name__)


# HTML template for standalone reports
# Uses minimal inline CSS for portability (no external dependencies)
HTML_TEMPLATE = """<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AutoML Report - {title}</title>
    <style>
        :root {{
            --primary: #2563eb;
            --success: #16a34a;
            --warning: #d97706;
            --danger: #dc2626;
            --bg: #f8fafc;
            --card-bg: #ffffff;
            --text: #1e293b;
            --text-muted: #64748b;
            --border: #e2e8f0;
        }}
        * {{ box-sizing: border-box; margin: 0; padding: 0; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
        }}
        .container {{ max-width: 1200px; margin: 0 auto; }}
        h1 {{ color: var(--primary); margin-bottom: 0.5rem; }}
        h2 {{ color: var(--text); margin: 1.5rem 0 1rem; border-bottom: 2px solid var(--border); padding-bottom: 0.5rem; }}
        h3 {{ color: var(--text-muted); margin: 1rem 0 0.5rem; }}
        .subtitle {{ color: var(--text-muted); margin-bottom: 2rem; }}
        .card {{
            background: var(--card-bg);
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }}
        .grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; }}
        .stat {{
            text-align: center;
            padding: 1rem;
            background: var(--bg);
            border-radius: 6px;
        }}
        .stat-value {{ font-size: 1.75rem; font-weight: bold; color: var(--primary); }}
        .stat-label {{ font-size: 0.875rem; color: var(--text-muted); }}
        table {{ width: 100%; border-collapse: collapse; margin-top: 1rem; }}
        th, td {{ padding: 0.75rem; text-align: left; border-bottom: 1px solid var(--border); }}
        th {{ background: var(--bg); font-weight: 600; color: var(--text-muted); }}
        tr:hover {{ background: var(--bg); }}
        .badge {{
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 9999px;
            font-size: 0.75rem;
            font-weight: 500;
        }}
        .badge-success {{ background: #dcfce7; color: var(--success); }}
        .badge-primary {{ background: #dbeafe; color: var(--primary); }}
        .badge-warning {{ background: #fef3c7; color: var(--warning); }}
        .best-row {{ background: #f0fdf4 !important; }}
        .param-list {{ font-family: monospace; font-size: 0.875rem; color: var(--text-muted); }}
        .warning {{ color: var(--warning); font-size: 0.875rem; }}
        .footer {{ margin-top: 2rem; text-align: center; color: var(--text-muted); font-size: 0.875rem; }}
        code {{ background: var(--bg); padding: 0.125rem 0.375rem; border-radius: 4px; font-size: 0.875rem; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ AutoML Report</h1>
        <p class="subtitle">Generated on {timestamp}</p>

        {content}

        <div class="footer">
            <p>Generated by <strong>mlcli-toolkit</strong> AutoML</p>
        </div>
    </div>
</body>
</html>
"""


class AutoMLReporter:
    """
    Reporter for AutoML classification results.

    Generates formatted output for console, JSON, and HTML.
    Works with any object that provides a `get_summary()` method
    returning the standard AutoML summary dictionary.

    Attributes:
        automl: The fitted AutoMLClassifier instance
        summary: Cached summary dictionary from automl.get_summary()

    Example:
        >>> from mlcli.automl import AutoMLClassifier
        >>> from mlcli.automl.reporter import AutoMLReporter
        >>>
        >>> automl = AutoMLClassifier(metric="accuracy")
        >>> automl.fit(X_train, y_train)
        >>>
        >>> reporter = AutoMLReporter(automl)
        >>> reporter.print_summary()  # Rich console output
        >>> reporter.save_json("report.json")
        >>> reporter.save_html("report.html")
    """

    def __init__(
        self,
        automl: Any,
        title: Optional[str] = None,
    ) -> None:
        """
        Initialize reporter with a fitted AutoML instance.

        Args:
            automl: Fitted AutoMLClassifier (or any object with get_summary())
            title: Optional title for reports (defaults to "AutoML Classification")

        Raises:
            ValueError: If automl doesn't have get_summary() method
            RuntimeError: If automl is not fitted
        """
        if not hasattr(automl, "get_summary"):
            raise ValueError(
                "automl must have a get_summary() method. "
                "Expected AutoMLClassifier or compatible object."
            )

        # Check if fitted
        if hasattr(automl, "is_fitted_") and not automl.is_fitted_:
            raise RuntimeError("AutoML is not fitted. Call fit() before creating a report.")

        self.automl = automl
        self.title = title or "AutoML Classification"

        # Cache summary for consistent reporting
        self._summary: Optional[Dict[str, Any]] = None

        logger.debug(f"AutoMLReporter initialized with title='{self.title}'")

    @property
    def summary(self) -> Dict[str, Any]:
        """
        Get the AutoML summary (cached for consistency).

        Returns:
            Dictionary containing full AutoML run summary
        """
        if self._summary is None:
            self._summary = self.automl.get_summary()
        return self._summary

    def refresh(self) -> "AutoMLReporter":
        """
        Refresh the cached summary from the AutoML instance.

        Use this if the AutoML state has changed since reporter creation.

        Returns:
            self for method chaining
        """
        self._summary = self.automl.get_summary()
        return self

    # Console output(Rich)

    def print_summary(self, show_params: bool = True) -> None:
        """
        Print a formatted summary to the console using Rich.

        Displays:
        - Header with run info
        - Data summary
        - Preprocessing info
        - Leaderboard table
        - Best model details

        Args:
            show_params: Whether to show hyperparameters in leaderboard
        """
        try:
            from rich.console import Console
            from rich.table import Table
            from rich.panel import Panel
            from rich.text import Text
        except ImportError:
            # Fallback to plain print if Rich is not available
            self._print_summary_plain()
            return

        console = Console()
        summary = self.summary

        # Header Panel
        config = summary.get("config", {})
        header_text = Text()
        header_text.append("ü§ñ AutoML Classification Report\n\n", style="bold blue")
        header_text.append("Metric: ", style="dim")
        header_text.append(f"{config.get('metric', 'N/A')}\n", style="cyan")
        header_text.append("Time Budget: ", style="dim")
        budget = config.get("time_budget_minutes")
        header_text.append(f"{budget} min\n" if budget else "Unlimited\n", style="cyan")
        header_text.append("Duration: ", style="dim")
        duration = summary.get("duration_seconds", 0)
        header_text.append(f"{duration:.1f}s\n" if duration else "N/A\n", style="cyan")
        header_text.append("Models Evaluated: ", style="dim")
        header_text.append(f"{summary.get('models_evaluated', 0)}", style="cyan")

        console.print(Panel(header_text, title=self.title, border_style="blue"))

        # Data Summary
        data = summary.get("data", {})
        if data.get("n_samples"):
            console.print("\n[bold]üìä Data Summary[/bold]")
            data_table = Table(show_header=False, box=None, padding=(0, 2))
            data_table.add_column("Label", style="dim")
            data_table.add_column("Value", style="cyan")

            data_table.add_row("Samples", str(data.get("n_samples", "N/A")))
            data_table.add_row("Features", str(data.get("n_features", "N/A")))
            data_table.add_row("Numeric", str(data.get("n_numeric", "N/A")))
            data_table.add_row("Categorical", str(data.get("n_categorical", "N/A")))

            imbalanced = data.get("is_imbalanced")
            if imbalanced:
                data_table.add_row("Imbalanced", "[yellow]Yes[/yellow]")
            else:
                data_table.add_row("Imbalanced", "No")

            console.print(data_table)

        # Preprocessing
        preprocessing = summary.get("preprocessing", {})
        if preprocessing.get("n_steps", 0) > 0:
            console.print("\n[bold]‚öôÔ∏è Preprocessing[/bold]")
            prep_table = Table(show_header=False, box=None, padding=(0, 2))
            prep_table.add_column("Label", style="dim")
            prep_table.add_column("Value", style="cyan")

            prep_table.add_row("Scaler", preprocessing.get("scaler_method", "None"))
            prep_table.add_row("Steps", str(preprocessing.get("n_steps", 0)))

            if preprocessing.get("has_missing_values"):
                strategy = preprocessing.get("missing_strategy", "N/A")
                prep_table.add_row("Missing Values", f"[yellow]Yes[/yellow] ({strategy})")

            console.print(prep_table)

            # Warnings
            warnings = preprocessing.get("warnings", [])
            for warn in warnings:
                console.print(f"  [yellow]‚ö† {warn}[/yellow]")

        # Leaderboard
        leaderboard = summary.get("leaderboard", [])
        if leaderboard:
            console.print("\n[bold]üèÜ Leaderboard[/bold]")

            lb_table = Table(show_header=True, header_style="bold")
            lb_table.add_column("Rank", justify="center", style="dim", width=6)
            lb_table.add_column("Model", style="cyan")
            lb_table.add_column("Framework", style="dim")
            lb_table.add_column("Score", justify="right", style="green")
            lb_table.add_column("Duration", justify="right", style="dim")

            if show_params:
                lb_table.add_column("Top Parameters", style="dim", max_width=40)

            for entry in leaderboard:
                rank = entry.get("rank", "?")
                model = entry.get("model_name", "Unknown")
                framework = entry.get("framework", "N/A")
                score = entry.get("score", 0)
                duration = entry.get("duration_seconds", 0)

                # Format rank with medal for top 3
                if rank == 1:
                    rank_str = "ü•á 1"
                elif rank == 2:
                    rank_str = "ü•à 2"
                elif rank == 3:
                    rank_str = "ü•â 3"
                else:
                    rank_str = f"   {rank}"

                row = [
                    rank_str,
                    model,
                    framework,
                    f"{score:.4f}",
                    f"{duration:.1f}s",
                ]

                if show_params:
                    params = entry.get("params", {})
                    # Show top 3 params
                    param_strs = [f"{k}={v}" for k, v in list(params.items())[:3]]
                    row.append(", ".join(param_strs) if param_strs else "defaults")

                lb_table.add_row(*row)

            console.print(lb_table)

        # Best Model
        best = summary.get("best_model", {})
        if best.get("name"):
            console.print("\n[bold]‚≠ê Best Model[/bold]")

            best_panel_text = Text()
            best_panel_text.append("Model: ", style="dim")
            best_panel_text.append(f"{best.get('name')}\n", style="bold green")
            best_panel_text.append(f"Score ({config.get('metric', 'score')}): ", style="dim")
            best_panel_text.append(f"{best.get('score', 0):.4f}\n", style="bold green")

            # Show params
            params = best.get("params", {})
            if params:
                best_panel_text.append("\nHyperparameters:\n", style="dim")
                for k, v in params.items():
                    if isinstance(v, float):
                        best_panel_text.append(f"  {k}: {v:.6g}\n", style="cyan")
                    else:
                        best_panel_text.append(f"  {k}: {v}\n", style="cyan")

            console.print(Panel(best_panel_text, border_style="green"))

        console.print()

    def _print_summary_plain(self) -> None:
        # Fallback plain-text summary when Rich is not available.
        summary = self.summary

        print("\n" + "=" * 60)
        print("AutoML Classification Report")
        print("=" * 60)

        config = summary.get("config", {})
        print(f"\nMetric: {config.get('metric', 'N/A')}")
        print(f"Duration: {summary.get('duration_seconds', 0):.1f}s")
        print(f"Models Evaluated: {summary.get('models_evaluated', 0)}")

        # Leaderboard
        leaderboard = summary.get("leaderboard", [])
        if leaderboard:
            print("\nLeaderboard:")
            print("-" * 50)
            for entry in leaderboard:
                print(
                    f"  {entry.get('rank')}. {entry.get('model_name')}: "
                    f"{entry.get('score', 0):.4f}"
                )

        # Best model
        best = summary.get("best_model", {})
        if best.get("name"):
            print(f"\nBest Model: {best.get('name')}")
            print(f"Best Score: {best.get('score', 0):.4f}")

        print("=" * 60 + "\n")

    def print_leaderboard(self, top_n: Optional[int] = None) -> None:
        """
        Print just the leaderboard table.

        Args:
            top_n: Limit to top N models (None for all)
        """
        try:
            from rich.console import Console
            from rich.table import Table
        except ImportError:
            self._print_leaderboard_plain(top_n)
            return

        console = Console()
        leaderboard = self.summary.get("leaderboard", [])

        if top_n:
            leaderboard = leaderboard[:top_n]

        table = Table(title="üèÜ AutoML Leaderboard", show_header=True)
        table.add_column("Rank", justify="center", width=6)
        table.add_column("Model", style="cyan")
        table.add_column("Score", justify="right", style="green")
        table.add_column("Duration", justify="right")

        for entry in leaderboard:
            table.add_row(
                str(entry.get("rank", "?")),
                entry.get("model_name", "Unknown"),
                f"{entry.get('score', 0):.4f}",
                f"{entry.get('duration_seconds', 0):.1f}s",
            )

        console.print(table)

    def _print_leaderboard_plain(self, top_n: Optional[int] = None) -> None:
        # Plain-text leaderboard fallback.
        leaderboard = self.summary.get("leaderboard", [])
        if top_n:
            leaderboard = leaderboard[:top_n]

        print("\nLeaderboard:")
        for entry in leaderboard:
            print(
                f"  {entry.get('rank')}. {entry.get('model_name')}: "
                f"{entry.get('score', 0):.4f} ({entry.get('duration_seconds', 0):.1f}s)"
            )

    # JSON Export
    def to_json(self, indent: int = 2) -> str:
        """
        Convert report to JSON string.

        Args:
            indent: JSON indentation level (0 for compact)

        Returns:
            JSON string of the report
        """
        report = self._build_report_dict()
        return json.dumps(report, indent=indent if indent > 0 else None, default=str)

    def save_json(self, filepath: Union[str, Path]) -> Path:
        """
        Save report as JSON file.

        Args:
            filepath: Path to save JSON file

        Returns:
            Path to saved file
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(self.to_json())

        logger.info(f"Saved JSON report to {filepath}")
        return filepath

    # HTML Export

    def to_html(self) -> str:
        """
        Generate standalone HTML report.

        Returns:
            Complete HTML document as string
        """
        summary = self.summary
        content_parts = []

        # Configuration Card
        config = summary.get("config", {})
        content_parts.append(self._html_section("Configuration", self._html_config_card(config)))

        # Data Summary Card
        data = summary.get("data", {})
        if data.get("n_samples"):
            content_parts.append(self._html_section("Data Summary", self._html_data_card(data)))

        # Preprocessing Card
        preprocessing = summary.get("preprocessing", {})
        if preprocessing.get("n_steps", 0) > 0:
            content_parts.append(
                self._html_section("Preprocessing", self._html_preprocessing_card(preprocessing))
            )

        # Leaderboard
        leaderboard = summary.get("leaderboard", [])
        if leaderboard:
            content_parts.append(
                self._html_section("Model Leaderboard", self._html_leaderboard_table(leaderboard))
            )

        # Best Model
        best = summary.get("best_model", {})
        if best.get("name"):
            content_parts.append(
                self._html_section("Best Model", self._html_best_model_card(best, config))
            )

        content = "\n".join(content_parts)

        return HTML_TEMPLATE.format(
            title=self.title,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            content=content,
        )

    def save_html(self, filepath: Union[str, Path]) -> Path:
        """
        Save report as HTML file.

        Args:
            filepath: Path to save HTML file

        Returns:
            Path to saved file
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(self.to_html())

        logger.info(f"Saved HTML report to {filepath}")
        return filepath

    # HTML Helpers
    def _html_section(self, title: str, content: str) -> str:
        # Wrap content in a section with title.
        return f'<h2>{title}</h2>\n<div class="card">\n{content}\n</div>'

    def _html_config_card(self, config: Dict[str, Any]) -> str:
        # Generate config summary as HTML.
        summary = self.summary
        duration = summary.get("duration_seconds", 0)

        return f"""
        <div class="grid">
            <div class="stat">
                <div class="stat-value">{config.get('metric', 'N/A')}</div>
                <div class="stat-label">Optimization Metric</div>
            </div>
            <div class="stat">
                <div class="stat-value">{summary.get('models_evaluated', 0)}</div>
                <div class="stat-label">Models Evaluated</div>
            </div>
            <div class="stat">
                <div class="stat-value">{duration:.1f}s</div>
                <div class="stat-label">Total Duration</div>
            </div>
            <div class="stat">
                <div class="stat-value">{config.get('cv', 5)}</div>
                <div class="stat-label">CV Folds</div>
            </div>
        </div>
        <h3>Settings</h3>
        <table>
            <tr><td>Time Budget</td><td>{config.get('time_budget_minutes', 'Unlimited')} min</td></tr>
            <tr><td>Max Models</td><td>{config.get('max_models', 'N/A')}</td></tr>
            <tr><td>Trials per Model</td><td>{config.get('n_trials_per_model', 'N/A')}</td></tr>
            <tr><td>Tuning Method</td><td>{config.get('tuning_method', 'N/A')}</td></tr>
            <tr><td>Auto Preprocess</td><td>{'Yes' if config.get('auto_preprocess') else 'No'}</td></tr>
        </table>
        """

    def _html_data_card(self, data: Dict[str, Any]) -> str:
        # Generate data summary as HTML.
        imbalanced_badge = ""
        if data.get("is_imbalanced"):
            imbalanced_badge = '<span class="badge badge-warning">Imbalanced</span>'

        return f"""
        <div class="grid">
            <div class="stat">
                <div class="stat-value">{data.get('n_samples', 'N/A')}</div>
                <div class="stat-label">Samples</div>
            </div>
            <div class="stat">
                <div class="stat-value">{data.get('n_features', 'N/A')}</div>
                <div class="stat-label">Features</div>
            </div>
            <div class="stat">
                <div class="stat-value">{data.get('n_numeric', 0)}</div>
                <div class="stat-label">Numeric</div>
            </div>
            <div class="stat">
                <div class="stat-value">{data.get('n_categorical', 0)}</div>
                <div class="stat-label">Categorical</div>
            </div>
        </div>
        {imbalanced_badge}
        """

    def _html_preprocessing_card(self, preprocessing: Dict[str, Any]) -> str:
        # Generate preprocessing summary as HTML.
        steps_html = ""
        for step in preprocessing.get("steps", []):
            steps_html += f"""
            <tr>
                <td>{step.get('name', 'N/A')}</td>
                <td><code>{step.get('method', 'N/A')}</code></td>
                <td>{step.get('reason', '')}</td>
            </tr>
            """

        warnings_html = ""
        for warn in preprocessing.get("warnings", []):
            warnings_html += f'<p class="warning">‚ö†Ô∏è {warn}</p>'

        missing_info = ""
        if preprocessing.get("has_missing_values"):
            missing_info = f"""
            <p class="warning">
                ‚ö†Ô∏è Missing values detected. Recommended strategy:
                <code>{preprocessing.get('missing_strategy', 'N/A')}</code>
            </p>
            """

        return f"""
        <table>
            <tr><td>Scaler</td><td><code>{preprocessing.get('scaler_method', 'None')}</code></td></tr>
            <tr><td>Total Steps</td><td>{preprocessing.get('n_steps', 0)}</td></tr>
        </table>
        {missing_info}
        {warnings_html}
        {'<h3>Steps</h3><table><tr><th>Name</th><th>Method</th><th>Reason</th></tr>' + steps_html + '</table>' if steps_html else ''}
        """

    def _html_leaderboard_table(self, leaderboard: List[Dict[str, Any]]) -> str:
        # Generate leaderboard as HTML table.
        rows = ""
        for entry in leaderboard:
            rank = entry.get("rank", "?")

            # Medal for top 3
            if rank == 1:
                rank_display = "ü•á 1"
                row_class = "best-row"
            elif rank == 2:
                rank_display = "ü•à 2"
                row_class = ""
            elif rank == 3:
                rank_display = "ü•â 3"
                row_class = ""
            else:
                rank_display = str(rank)
                row_class = ""

            # Format params
            params = entry.get("params", {})
            params_display = ", ".join(
                f"{k}={v:.4g}" if isinstance(v, float) else f"{k}={v}"
                for k, v in list(params.items())[:3]
            )
            if not params_display:
                params_display = "defaults"

            rows += f"""
            <tr class="{row_class}">
                <td><strong>{rank_display}</strong></td>
                <td>{entry.get('model_name', 'Unknown')}</td>
                <td><span class="badge badge-primary">{entry.get('framework', 'N/A')}</span></td>
                <td><strong>{entry.get('score', 0):.4f}</strong></td>
                <td>{entry.get('duration_seconds', 0):.1f}s</td>
                <td class="param-list">{params_display}</td>
            </tr>
            """

        return f"""
        <table>
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Framework</th>
                    <th>Score</th>
                    <th>Duration</th>
                    <th>Parameters</th>
                </tr>
            </thead>
            <tbody>
                {rows}
            </tbody>
        </table>
        """

    def _html_best_model_card(
        self,
        best: Dict[str, Any],
        config: Dict[str, Any],
    ) -> str:
        # Generate best model summary as HTML.
        params_html = ""
        for k, v in best.get("params", {}).items():
            if isinstance(v, float):
                params_html += f"<tr><td><code>{k}</code></td><td>{v:.6g}</td></tr>"
            else:
                params_html += f"<tr><td><code>{k}</code></td><td>{v}</td></tr>"

        return f"""
        <div class="grid">
            <div class="stat">
                <div class="stat-value" style="color: var(--success);">{best.get('name', 'N/A')}</div>
                <div class="stat-label">Best Model</div>
            </div>
            <div class="stat">
                <div class="stat-value" style="color: var(--success);">{best.get('score', 0):.4f}</div>
                <div class="stat-label">{config.get('metric', 'Score')}</div>
            </div>
        </div>
        {'<h3>Hyperparameters</h3><table>' + params_html + '</table>' if params_html else ''}
        """

    # Internal Helpers

    def _build_report_dict(self) -> Dict[str, Any]:
        """
        Build complete report dictionary for export.

        Returns:
            Dictionary with report metadata and AutoML summary
        """
        return {
            "report_metadata": {
                "title": self.title,
                "generated_at": datetime.now().isoformat(),
                "generator": "mlcli-toolkit AutoMLReporter",
            },
            "summary": self.summary,
        }

    def __repr__(self) -> str:
        summary = self.summary
        return (
            f"AutoMLReporter("
            f"models={summary.get('models_evaluated', 0)}, "
            f"best='{summary.get('best_model', {}).get('name', 'N/A')}', "
            f"score={summary.get('best_model', {}).get('score', 0):.4f})"
        )
